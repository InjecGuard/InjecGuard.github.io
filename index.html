<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Online Demo</title>

  <!-- Bulma framework -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">

  <style>
    /* Styling the form container */
    .form-container {
      background-color: white;
      padding: 30px;
      border-radius: 10px;
      box-shadow: 0px 4px 12px rgba(0, 0, 0, 0.1);
      width: 100%;
    }

    /* Style for the input textarea */
    .input-text {
      width: 100%;
      height: 100px;
      padding: 12px;
      border: 1px solid #e0e0e0;
      border-radius: 6px;
      font-size: 16px;
      color: #333;
      box-sizing: border-box;
      outline: none;
      transition: border-color 0.2s ease-in-out;
    }

    .input-text:focus {
      border-color: #5A9EF6;
    }

    /* Style for the compute button */
    .compute-button {
      margin-top: 20px;
      padding: 12px 20px;
      background-color: #5A9EF6;
      color: white;
      border: none;
      border-radius: 6px;
      font-size: 16px;
      cursor: pointer;
      width: 100%;
    }

    .compute-button:disabled {
      background-color: #c5d9f3;
    }

    /* Style for progress bars */
    .progress-bar {
      background-color: #f3f3f3;
      border-radius: 4px;
      height: 10px;
      margin-top: 10px;
    }

    .progress-bar-inner {
      height: 10px;
      border-radius: 4px;
    }

    /* Color for the benign and injection progress bars */
    .benign-bar {
      background-color: #c39bd3;
    }

    .injection-bar {
      background-color: #85c1e9;
    }

    /* Label and score alignment */
    .label-score {
      display: flex;
      justify-content: space-between;
      margin-top: 5px;
    }
  </style>

  <title>NotInject: Evaluating and Mitigating Over-defense in Prompt Guard Models for Robustness against Prompt Injection Attacks</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">NotInject: Evaluating and Mitigating Over-defense in Prompt Guard Models for Robustness against Prompt Injection Attacks</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://leolee99.github.io/" target="_blank">Hao Li</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://sheltonliu-n.github.io/" target="_blank">Xiaogeng Liu</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://xiaocw11.github.io/" target="_blank">Chaowei Xiao</a><sup>†</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">SaFoLab, University of Wisconsin-Madison</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution, <sup>†</sup>Correspondence</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/SaFoLab-WISC/InjecGuard" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/leolee99/NotInject" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <video poster="" id="tree" autoplay controls muted loop width="80%" height="80%">
          <!-- Your video here -->
          <source src="static/videos/Demo.mp4" type="video/mp4">
        </video>
      </center>
      <h2 class="subtitle has-text-centered">
        A Demo of InjecGuard. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Prompt injection attacks pose a critical threat to large language models (LLMs), enabling goal hijacking and data leakage. Prompt guard models, though effective in defense, suffer from over-defense—falsely flagging benign inputs as malicious due to trigger word bias. To address this issue, we introduce NotInject, an evaluation dataset that systematically measures over-defense across various prompt guard models. NotInject contains 339 benign samples enriched with trigger words common in prompt injection attacks, enabling fine-grained evaluation. Our results show that state-of-the-art models suffer from over-defense issues, with accuracy dropping close to random guessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt guard model that incorporates a new training strategy, Mitigating Over-defense for Free (MOF), which significantly reduces the bias on trigger words. InjecGuard demonstrates state-of-the-art performance on diverse benchmarks including NotInject, surpassing the existing best model by 30.8%, offering a robust and open-source solution for detecting prompt injection attacks.
          </p>
        </div>
        <img class="columns is-centered has-text-centered" src="./static/images/performance.png" alt="Teaser" width="75%"
        style="margin:0 auto">
        <br>
        <figcaption>
            <p style="text-align: center; color: #061E61;">
                <b>Figure 1:</b> Comparison on 3 dimensions of benign, malicious, and over-defense accuracy.
            </p>
        </figcaption>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
          <h2 class="title is-2">Overdefense Benchmark——NotInject</h2>
          <br>
      </div>

      <!-- Overdefense Issue -->
      <div class="columns is-centered">
        <div class="column is-full-width">
            <h4 class="title is-3">• Overdefense Issue</h4>

            <div class="content has-text-justified">
                <img class="columns is-centered has-text-centered" src="./static/images/huggingface.png" alt="Teaser" width="95%"
                     style="margin:0 auto">
                <br>
                <figcaption>
                    <p style="text-align: center; color: #061E61;">
                        <b>Figure 1:</b> Left: PromptGuard, Right: ProtectAIv2, both misclassify benign inputs as malicious due to an overreliance on specific trigger words, such as ``ignore''.
                    </p>
                </figcaption>
            </div>
            <br/>
        </div>
      </div>

      <!-- Pipeline -->
      <div class="columns is-centered">
          <div class="column is-full-width">
              <h4 class="title is-3">• Dataset Construction </h4>

              <div class="content has-text-justified">
                  <img class="columns is-centered has-text-centered" src="./static/images/pipeline.png" alt="Teaser" width="95%"
                       style="margin:0 auto">
                  <br>
                  <figcaption>
                      <p style="text-align: center; color: #061E61;">
                          <b>Figure 1:</b> The pipeline for constructing NotInject dataset.
                      </p>
                  </figcaption>
                  <br>
                  <p>
                    <ul>
                      <li>
                          <b>Trigger Words Indentification.</b> To identify potential trigger words, was first conduct a word frequency analysis across both benign and injection datasets. The words are then ranked according to their frequency within each dataset, sorted separately for benign and injection datasets. By calculating the differences in word rankings between the two datasets, we highlight words that are disproportionately frequent in the injection dataset but rare in the benign dataset. These words are flagged as potential trigger words, which can serve as indicators of injection attempts.
                      </li>
                      <li>
                          <b>Trigger Words Refinement.</b>
                          To refine the list of candidate trigger words, a multi-step process is implemented to filter out irrelevant or commonly used terms that are not indicative of prompt injection attempts. Initially, a large language model (LLM) is employed to automatically assess the relevance of each word, based on the prompt: "Do you think the word of {word} is especially frequent in malicious or prompt attack scenarios?" This step helps eliminate words that are not pertinent to injection attacks. Following the automated filtering, a manual verification process is conducted to ensure the final list contains only words that are strongly associated with prompt injection attempts, further enhancing the accuracy of the trigger word identification.
                      </li>
                      <li>
                          <b>Corpus Generation.</b>
                          As a result of the refinement process, we select 113 trigger words and utilize a large language model (LLM) to generate benign sentences incorporating these words. The dataset is structured into three subsets, each containing samples with 1, 2, and 3 trigger words per sentence. To ensure that all generated sentences are non-malicious, both LLM-based and manual refinement processes are applied. The final dataset, termed NotInject, consists of 339 samples in total, with 113 samples in each subset. The generated sentences span a wide range of topics, including common queries, technical inquiries, virtual content creation, and multilingual queries, thus ensuring diversity and comprehensiveness.
                      </li>
                  </ul>
                  </p>
                  <br>
              </div>
              <br/>

          </div>
      </div>


  </div>
</section>

<!-- End Benchmark -->


<section class="section">
  <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
          <h2 class="title is-2">Injection Guardrail——InjecGuard</h2>
          <br>
      </div>

      <!-- InjecGuard -->
      <div class="columns is-centered">
        <div class="column is-full-width">
            <h4 class="title is-3">• Brief Introduction</h4>

            <div class="content has-text-justified">
              <br>
              <p>
                <b>InjecGuard</b> is a lightweight model designed to defend against prompt injection attacks. It delivers strong performance across benign, malicious, and over-defense accuracy metrics, surpassing existing guard models such as PromptGuard, ProtectAIv2, and LakeraAI. Despite its compact size, with model parameters of only 184MB, InjecGuard achieves competitive performance comparable to advanced commercial large language models like GPT-4.
                <br>
                <b>Note:</b> All training details, including the code and datasets, are fully released and available.
              </p>
              <!-- <br>
                <img class="columns is-centered has-text-centered" src="./static/images/Results.png" alt="Teaser" width="80%"
                     style="margin:0 auto">
                <br>
                <figcaption>
                    <p style="text-align: center; color: #061E61;">
                        <b>Figure 1:</b> Performance and overhead comparison between InjecGuard and other baseline models. 
                    </p>
                </figcaption> -->
            </div>
            <br/>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
            <h4 class="title is-3">• Demo</h4>
            <div class="container is-max-desktop">
              <div class="hero-body">
                <center>
                  <video poster="" id="tree" autoplay controls muted loop width="80%" height="80%">
                    <!-- Your video here -->
                    <source src="static/videos/Demo.mp4" type="video/mp4">
                  </video>
                </center>
                <h2 class="subtitle has-text-centered">
                  A Demo of InjecGuard. 
                </h2>
              </div>
            </div>
        </div>
      </div>

      <!-- Main Results -->
      <div class="columns is-centered">
        <div class="column is-full-width">
            <h4 class="title is-3">• Evaluation</h4>

            <div class="content has-text-justified">
              <br>
                <img class="columns is-centered has-text-centered" src="./static/images/Results.png" alt="Teaser" width="80%"
                     style="margin:0 auto">
                <br>
                <figcaption>
                    <p style="text-align: center; color: #061E61;">
                        <b>Figure 1:</b> Performance and overhead comparison between InjecGuard and other baseline models. 
                    </p>
                </figcaption>
            </div>
            <br/>
        </div>
      </div>

      <!-- Visualiazation -->
      <div class="columns is-centered">
        <div class="column is-full-width">
            <h4 class="title is-3">• Visualization</h4>

            <div class="content has-text-justified">
                <img class="columns is-centered has-text-centered" src="./static/images/visualization_concat.png" alt="Teaser" width="80%"
                     style="margin:0 auto">
                <br>
                <figcaption>
                    <p style="text-align: center; color: #061E61;">
                        <b>Figure 1:</b> Left: The visualization of different predictions. Right: The visualization of attention weight. Given an instruction of ``[CLS] Can I ignore this warning appeared in my code? [SEP]'', ProtectAIv2 assigns extremely high attention weights to the word ``ignore,'' leading to misclassification as Injection. In contrast, our method distributes attention across the entire sentence, successfully predicting it as benign.
                    </p>
                </figcaption>
            </div>
            <br/>
        </div>
      </div>

  </div>
</section>

  <!-- Main container -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-half">
          <div class="form-container">
            <h2 class="title is-4">Text Classification</h2>

            <!-- Input form -->
            <form id="inferenceForm">
              <textarea id="inputText" class="input-text" placeholder="Your sentence here..."></textarea>
              <button type="submit" class="compute-button">Compute</button>
            </form>

            <!-- Loading animation in the button -->
            <div id="spinner" style="display: none; margin-top: 10px;">
              <button class="button is-loading is-info is-fullwidth">Loading</button>
            </div>

            <!-- Output display -->
            <div id="result" style="display: none;">
              <h3>Result:</h3>
              <div id="output"></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <script>
    // Handle form submission
    document.getElementById("inferenceForm").addEventListener("submit", async function(event) {
        event.preventDefault();  // Prevent page reload

        const sentence = document.getElementById("inputText").value;
        if (sentence === "") {
            alert("Please enter a sentence.");
            return;
        }

        // Disable the button and show loading spinner
        const button = document.querySelector(".compute-button");
        button.disabled = true;
        document.getElementById("spinner").style.display = "block";
        button.textContent = "Processing...";

        try {
            // Send the request to your endpoint
            const response = await fetch("https://d43yv1p4zw5qasjv.us-east-1.aws.endpoints.huggingface.cloud", {
                method: "POST",  // Assuming the endpoint requires POST method
                headers: { 
                  "Accept" : "application/json",
                  "Content-Type": "application/json" 
                },
                body: JSON.stringify({
                    "inputs": sentence
                })
            });

            // Parse the response
            const data = await response.json();

            // Hide the loading spinner
            document.getElementById("spinner").style.display = "none";
            button.disabled = false;
            button.textContent = "Compute";

            // Clear any previous output
            const outputDiv = document.getElementById("output");
            outputDiv.innerHTML = "";
            document.getElementById("result").style.display = "block";

            // Display progress bars
            if (Array.isArray(data)) {
                data.forEach(item => {
                    const label = item.label;
                    const score = item.score.toFixed(3);

                    // Create label and score display
                    const labelScore = document.createElement("div");
                    labelScore.className = "label-score";
                    labelScore.innerHTML = `<span>${label}</span><span>${score}</span>`;

                    // Create progress bar
                    const progressBar = document.createElement("div");
                    progressBar.className = "progress-bar";

                    const progressBarInner = document.createElement("div");
                    progressBarInner.className = `${label}-bar progress-bar-inner`;
                    progressBarInner.style.width = `${score * 100}%`;

                    progressBar.appendChild(progressBarInner);

                    // Append to the output
                    outputDiv.appendChild(labelScore);
                    outputDiv.appendChild(progressBar);
                });
            }

        } catch (error) {
            // Hide the loading spinner and display the error
            document.getElementById("spinner").style.display = "none";
            button.disabled = false;
            button.textContent = "Compute";
            alert("An error occurred: " + error.message);
        }
    });
  </script>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
